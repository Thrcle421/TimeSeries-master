{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5030,
     "status": "ok",
     "timestamp": 1667983740592,
     "user": {
      "displayName": "Levi Ack",
      "userId": "16387227878980052345"
     },
     "user_tz": -480
    },
    "id": "adJRvQc-t0pt",
    "outputId": "605a8ecf-cb31-4aeb-e16b-26138425f8e3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scale data method--我们在论文中提出的按照七种不同缩放方法和四个不同处理方向的预处理策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# #######################\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "\n",
    "\n",
    "def ScaleData(train_x, scaling_method, dimension, random_seed):\n",
    "    if scaling_method == \"none\":\n",
    "        return train_x\n",
    "    if np.isinf(train_x).any():\n",
    "        print(\"Train or test set contains infinity.\")\n",
    "        exit(-1)\n",
    "    scaling_dict = {\n",
    "        \"minmax\": MinMaxScaler(),\n",
    "        \"maxabs\": MaxAbsScaler(),\n",
    "        \"standard\": StandardScaler(),\n",
    "        \"robust\": RobustScaler(),\n",
    "        \"quantile\": QuantileTransformer(random_state=random_seed),\n",
    "        \"powert\": PowerTransformer(),\n",
    "        \"normalize\": Normalizer(),\n",
    "    }\n",
    "    if scaling_method not in scaling_dict.keys():\n",
    "        print(f\"Scaling method {scaling_method} not found.\")\n",
    "        exit(-1)\n",
    "    if dimension not in [\"timesteps\", \"channels\", \"all\", \"both\"]:\n",
    "        print(f\"Dimension {dimension} not found.\")\n",
    "        exit(-1)\n",
    "\n",
    "    dim1 = -1\n",
    "    dim2 = 1\n",
    "    if scaling_method == \"normalize\":\n",
    "        dim1 = 1\n",
    "        dim2 = -1\n",
    "    out_train_x = np.zeros_like(train_x, dtype=np.float64)\n",
    "\n",
    "    train_shape = train_x.shape\n",
    "    if dimension == \"all\":\n",
    "        out_train_x = (\n",
    "            scaling_dict[scaling_method]\n",
    "            .fit_transform(train_x.reshape((dim1, dim2)))\n",
    "            .reshape(train_shape)\n",
    "        )\n",
    "    else:\n",
    "        if dimension == \"channels\":\n",
    "            train_channel_shape = train_x[:, 0, :].shape\n",
    "            for i in range(train_x.shape[1]):\n",
    "                out_train_x[:, i, :] = (\n",
    "                    scaling_dict[scaling_method]\n",
    "                    .fit_transform(train_x[:, i, :].reshape((dim1, dim2)))\n",
    "                    .reshape(train_channel_shape)\n",
    "                )\n",
    "\n",
    "        elif dimension == \"timesteps\":\n",
    "            train_timest_shape = train_x[:, :, 0].shape\n",
    "\n",
    "            for i in range(train_x.shape[2]):\n",
    "                out_train_x[:, :, i] = (\n",
    "                    scaling_dict[scaling_method]\n",
    "                    .fit_transform(train_x[:, :, i].reshape((dim1, dim2)))\n",
    "                    .reshape(train_timest_shape)\n",
    "                )\n",
    "\n",
    "        elif dimension == \"both\":\n",
    "            train_both_shape = train_x[:, 0, 0].shape\n",
    "            for i in range(train_x.shape[1]):\n",
    "                for j in range(train_x.shape[2]):\n",
    "                    out_train_x[:, i, j] = (\n",
    "                        scaling_dict[scaling_method]\n",
    "                        .fit_transform(train_x[:, i, j].reshape((dim1, dim2)))\n",
    "                        .reshape(train_both_shape)\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            print(f\"Dimension {dimension} not found.\")\n",
    "            exit(-1)\n",
    "    return out_train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import nacessray library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "!#Importing libraries\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use follow code to get ADNI Data and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as io\n",
    "\n",
    "y = io.loadmat(\"D:/机器学习前沿实验/实验课一/dataset/ADNI.mat\")\n",
    "##AD\n",
    "AD = np.asarray(y[\"AD\"])\n",
    "AD_label = np.zeros([AD.shape[0], 1], dtype=int)\n",
    "##MCI\n",
    "MCI = np.asarray(y[\"MCI\"])\n",
    "MCIn = np.asarray(y[\"MCIn\"])\n",
    "MCIp = np.asarray(y[\"MCIp\"])\n",
    "MCI = np.vstack((MCI, MCIn, MCIp))\n",
    "MCI_label = np.ones([MCI.shape[0], 1], dtype=int)\n",
    "##NC\n",
    "NC = np.asarray(y[\"NC\"])\n",
    "NC_label = np.full((NC.shape[0], 1), 2, dtype=int)\n",
    "\n",
    "##合并:\n",
    "print(AD_label.shape)\n",
    "# print(EMCI_lable.shape)\n",
    "# print(LMCI_lable.shape)\n",
    "print(MCI_label.shape)\n",
    "print(NC_label.shape)\n",
    "\n",
    "\n",
    "Data = np.vstack((AD, MCI, NC))\n",
    "Label = np.vstack((AD_label, MCI_label, NC_label))\n",
    "\n",
    "print(Data.shape)\n",
    "print(Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use follow code to get PPMI Data and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as io\n",
    "# import numpy as np\n",
    "# y=io.loadmat(\"D:/机器学习前沿实验/实验课一/dataset/PPMI.mat\")\n",
    "# NC=np.asarray(y['NC'])\n",
    "# NC_label=np.full((NC.shape[0],1),0,dtype=int)\n",
    "# PD=np.asarray(y['PD'])\n",
    "# PD_label=np.full((PD.shape[0],1),1,dtype=int)\n",
    "\n",
    "\n",
    "# Label_1=np.vstack((NC_label,PD_label))\n",
    "# Data=np.vstack((NC,PD))\n",
    "# Label=Label_1.reshape(Label_1.shape[0])\n",
    "\n",
    "# print(Data.shape)\n",
    "# print(Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer Data to three dimension so that we can use scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.expand_dims(Data, axis=-1)\n",
    "Data = Data.transpose(0, 2, 1)\n",
    "Data.shape, Label.shape\n",
    "\n",
    "\n",
    "##转换标签以适应神经网络结构\n",
    "Label = keras.utils.to_categorical(Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct the fcn neural net work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUbyxQp5tUUu"
   },
   "outputs": [],
   "source": [
    "class Classifier_FCN:\n",
    "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True):\n",
    "        self.output_directory = output_directory\n",
    "        if build == True:\n",
    "            self.model = self.build_model(input_shape, nb_classes)\n",
    "            if verbose == True:\n",
    "                self.model.summary()\n",
    "            self.verbose = verbose\n",
    "            self.model.save_weights(self.output_directory + \"ECG_FCN_model_init.hdf5\")\n",
    "        return\n",
    "\n",
    "    def build_model(self, input_shape, nb_classes):\n",
    "        input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "        conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding=\"same\")(input_layer)\n",
    "        conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "        conv1 = keras.layers.Activation(activation=\"relu\")(conv1)\n",
    "\n",
    "        conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding=\"same\")(conv1)\n",
    "        conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "        conv2 = keras.layers.Activation(\"relu\")(conv2)\n",
    "\n",
    "        conv3 = keras.layers.Conv1D(128, kernel_size=3, padding=\"same\")(conv2)\n",
    "        conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "        conv3 = keras.layers.Activation(\"relu\")(conv3)\n",
    "\n",
    "        gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "        output_layer = keras.layers.Dense(nb_classes, activation=\"softmax\")(gap_layer)\n",
    "\n",
    "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "        model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"lr\", factor=0.5, patience=50, min_lr=0.0001\n",
    "        )\n",
    "\n",
    "        file_path = self.output_directory + \"ECG_best_FCN_model.hdf5\"\n",
    "\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=file_path, monitor=\"loss\", save_best_only=True\n",
    "        )\n",
    "\n",
    "        # early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, baseline = 0.99)\n",
    "\n",
    "        self.callbacks = [model_checkpoint]  # , early_stopping] reduce_lr,\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, batch_size, nb_epochs):\n",
    "\n",
    "        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
    "        hist = self.model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=mini_batch_size,\n",
    "            epochs=nb_epochs,\n",
    "            verbose=self.verbose,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=self.callbacks,\n",
    "        )\n",
    "\n",
    "        self.model.save(self.output_directory + \"last_FCN_model.hdf5\")\n",
    "\n",
    "        model = keras.models.load_model(self.output_directory + \"best_FCN_model.hdf5\")\n",
    "\n",
    "        y_pred = model.predict(x_val)\n",
    "\n",
    "        # convert the predicted from binary to integer\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "    def predict(self, x_train, y_train, x_test, y_test):\n",
    "        model_path = self.output_directory + \"best_FCN_model.hdf5\"\n",
    "        model = keras.models.load_model(model_path)\n",
    "        y_pred_train = model.predict(x_train, verbose=1)\n",
    "        y_pred_test = model.predict(x_test, verbose=1)\n",
    "\n",
    "        return y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五折交叉验证，利用注释处不同代码获得不同效果/不同数据集/是否使用scaledata策略，scaledata策略参数等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=100)  # 5折交叉验证\n",
    "\n",
    "i = 1\n",
    "a = []\n",
    "X_1 = ScaleData(Data, \"quantile\", \"all\", 100)  # 使用scaledata策略\n",
    "# X_1=Data   #不使用scaledata策略\n",
    "for train_index, test_index in kf.split(X_1, Label):\n",
    "    print(\"\\n{} of kfold {}\".format(i, kf.n_splits))\n",
    "    X_train, X_test = X_1[train_index], X_1[test_index]\n",
    "    y_train, y_test = Label[train_index], Label[test_index]\n",
    "    fcn_classifier = Classifier_FCN(\n",
    "        os.getcwd(), X_train.shape[1:], nb_classes=y_train.shape[1], verbose=True\n",
    "    )\n",
    "    fcn_classifier.fit(X_train, y_train, X_test, y_test, 16, 3000)\n",
    "    i += 1\n",
    "    best_val_accuracy = np.asarray(fcn_classifier.model.history.history[\"val_accuracy\"])\n",
    "    print(\"best performance of this circle:)\", best_val_accuracy.max())\n",
    "    a.append(best_val_accuracy.max())\n",
    "#     model.save(f\"1dConv-Fold{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打印输出五折交叉验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.asarray(a)\n",
    "print(\"五折交叉验证结果为:\", b.mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1kg9dl7Pqx9rOB5YQbaxQbLJKOyFIkIeD",
     "timestamp": 1667549722356
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "420px",
    "left": "1542px",
    "right": "20px",
    "top": "120px",
    "width": "358px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "9454f094a5c1e8e6b91f3e6f0db0dad0551253d40628effd67a60000e11016e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
