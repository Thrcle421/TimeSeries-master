{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75521f46",
   "metadata": {},
   "source": [
    "## use follow code to get PPMI data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as io\n",
    "\n",
    "y = io.loadmat(\"D:\\\\机器学习前沿实验\\\\实验课一\\\\dataset\\\\PPMI.mat\")\n",
    "NC = np.asarray(y[\"NC\"])\n",
    "NC_label = np.full((NC.shape[0], 1), 0, dtype=int)\n",
    "PD = np.asarray(y[\"PD\"])\n",
    "PD_label = np.full((PD.shape[0], 1), 1, dtype=int)\n",
    "\n",
    "\n",
    "Label_1 = np.vstack((NC_label, PD_label))\n",
    "Data = np.vstack((NC, PD))\n",
    "Label = Label_1.reshape(Label_1.shape[0])\n",
    "\n",
    "print(Data.shape)\n",
    "print(Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacc523",
   "metadata": {},
   "source": [
    "## use follow code to get ADNI data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as io\n",
    "# import numpy as np\n",
    "# y=io.loadmat(\"D:\\\\机器学习前沿实验\\\\实验课一\\\\dataset\\\\ADNI.mat\")\n",
    "# ##AD\n",
    "# AD=np.asarray(y['AD'])\n",
    "# AD_label=np.zeros([AD.shape[0],1],dtype=int)\n",
    "# ##MCI\n",
    "# MCI=np.asarray(y['MCI'])\n",
    "# MCIn=np.asarray(y['MCIn'])\n",
    "# MCIp=np.asarray(y['MCIp'])\n",
    "# MCI=np.vstack((MCI,MCIn,MCIp))\n",
    "# MCI_label=np.ones([MCI.shape[0],1],dtype=int)\n",
    "# ##NC\n",
    "# NC=np.asarray(y['NC'])\n",
    "# NC_label=np.full((NC.shape[0],1),2,dtype=int)\n",
    "\n",
    "# ##合并:\n",
    "# print(AD_label.shape)\n",
    "# # print(EMCI_lable.shape)\n",
    "# # print(LMCI_lable.shape)\n",
    "# print(MCI_label.shape)\n",
    "# print(NC_label.shape)\n",
    "\n",
    "\n",
    "# Data=np.vstack((AD,MCI,NC))\n",
    "# Label=np.vstack((AD_label,MCI_label,NC_label))\n",
    "\n",
    "# print(Data.shape)\n",
    "# print(Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49579cc9",
   "metadata": {},
   "source": [
    "## import necessray library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00fc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import LSTM, BatchNormalization, Dense, Dropout\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch == 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 10)\n",
    "    # 每隔5个epoch，学习率减小为原来的1/10\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr * 10)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr changed to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "\n",
    "reduce_lr = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f08d78",
   "metadata": {},
   "source": [
    "## scale data method--我们在论文中提出的按照七种不同缩放方法和四个不同处理方向的预处理策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6db305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# #######################\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "\n",
    "\n",
    "def ScaleData(train_x, scaling_method, dimension, random_seed):\n",
    "    if scaling_method == \"none\":\n",
    "        return train_x\n",
    "    if np.isinf(train_x).any():\n",
    "        print(\"Train or test set contains infinity.\")\n",
    "        exit(-1)\n",
    "    scaling_dict = {\n",
    "        \"minmax\": MinMaxScaler(),\n",
    "        \"maxabs\": MaxAbsScaler(),\n",
    "        \"standard\": StandardScaler(),\n",
    "        \"robust\": RobustScaler(),\n",
    "        \"quantile\": QuantileTransformer(random_state=random_seed),\n",
    "        \"powert\": PowerTransformer(),\n",
    "        \"normalize\": Normalizer(),\n",
    "    }\n",
    "    if scaling_method not in scaling_dict.keys():\n",
    "        print(f\"Scaling method {scaling_method} not found.\")\n",
    "        exit(-1)\n",
    "    if dimension not in [\"timesteps\", \"channels\", \"all\", \"both\"]:\n",
    "        print(f\"Dimension {dimension} not found.\")\n",
    "        exit(-1)\n",
    "\n",
    "    dim1 = -1\n",
    "    dim2 = 1\n",
    "    if scaling_method == \"normalize\":\n",
    "        dim1 = 1\n",
    "        dim2 = -1\n",
    "    out_train_x = np.zeros_like(train_x, dtype=np.float64)\n",
    "\n",
    "    train_shape = train_x.shape\n",
    "    if dimension == \"all\":\n",
    "        out_train_x = (\n",
    "            scaling_dict[scaling_method]\n",
    "            .fit_transform(train_x.reshape((dim1, dim2)))\n",
    "            .reshape(train_shape)\n",
    "        )\n",
    "    else:\n",
    "        if dimension == \"channels\":\n",
    "            train_channel_shape = train_x[:, 0, :].shape\n",
    "            for i in range(train_x.shape[1]):\n",
    "                out_train_x[:, i, :] = (\n",
    "                    scaling_dict[scaling_method]\n",
    "                    .fit_transform(train_x[:, i, :].reshape((dim1, dim2)))\n",
    "                    .reshape(train_channel_shape)\n",
    "                )\n",
    "\n",
    "        elif dimension == \"timesteps\":\n",
    "            train_timest_shape = train_x[:, :, 0].shape\n",
    "\n",
    "            for i in range(train_x.shape[2]):\n",
    "                out_train_x[:, :, i] = (\n",
    "                    scaling_dict[scaling_method]\n",
    "                    .fit_transform(train_x[:, :, i].reshape((dim1, dim2)))\n",
    "                    .reshape(train_timest_shape)\n",
    "                )\n",
    "\n",
    "        elif dimension == \"both\":\n",
    "            train_both_shape = train_x[:, 0, 0].shape\n",
    "            for i in range(train_x.shape[1]):\n",
    "                for j in range(train_x.shape[2]):\n",
    "                    out_train_x[:, i, j] = (\n",
    "                        scaling_dict[scaling_method]\n",
    "                        .fit_transform(train_x[:, i, j].reshape((dim1, dim2)))\n",
    "                        .reshape(train_both_shape)\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            print(f\"Dimension {dimension} not found.\")\n",
    "            exit(-1)\n",
    "    return out_train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32f812",
   "metadata": {},
   "source": [
    "# transfer data to three dimension so that we can use scaledata method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee84d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.expand_dims(Data, axis=-1)\n",
    "Data = Data.transpose(0, 2, 1)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Label_new = to_categorical(Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dropout_rate=0.25, activation=\"relu\", num_class=2):\n",
    "    start_neurons = 512\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(start_neurons, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 2, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 4, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 8, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate / 2))\n",
    "\n",
    "    model.add(Dense(num_class, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",  # 加速神经网络\n",
    "        loss=\"categorical_crossentropy\",  # 损失函数\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36370158",
   "metadata": {},
   "source": [
    "## 使用下面的code来完成MLP的 有/无 scaledata 方法的五折交叉对比实验，可以通过调整使用注释处的code来完成选择是否使用scaledata，并输出五折交叉验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdc99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5折交叉验证\n",
    "\n",
    "i = 1\n",
    "a = []\n",
    "X = ScaleData(Data, \"quantile\", \"both\", 100)  ##使用scaledata方法\n",
    "# X=Data ##不使用scaledata方法\n",
    "for train_index, test_index in kf.split(X, Label_new):\n",
    "    print(\"\\n{} of kfold {}\".format(i, kf.n_splits))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Label_new[train_index], Label_new[test_index]\n",
    "    model = build_model()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=5,\n",
    "        epochs=300,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[reduce_lr],\n",
    "    )\n",
    "    x = (np.asarray(history.history[\"accuracy\"]),)\n",
    "    y = np.asarray(history.history[\"val_accuracy\"])\n",
    "    b = np.asarray(y)\n",
    "    a.append(b.max())\n",
    "    i += 1\n",
    "\n",
    "b = np.asarray(a)\n",
    "print(\"五折交叉验证结果为:\", b.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f9ec9",
   "metadata": {},
   "source": [
    "## 使用下面的code来完成SVM和ExtraTreesClassifier，GradientBoostingClassifier，RandomForestClassifier的 有/无 scaledata 方法的五折交叉对比实验，可以通过调整使用注释处的code来完成选择是否使用scaledata，并输出五折交叉验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5折交叉验证\n",
    "\n",
    "i = 1\n",
    "a = []\n",
    "\n",
    "X = ScaleData(Data, \"quantile\", \"both\", 100)\n",
    "\n",
    "X_1 = X.reshape((-1, NC.shape[1]))  ##使用这行代码获取进行scaledata方法后的数据进行训练\n",
    "# X_1=Data.reshape((-1,NC.shape[1])) ##使用这行代码获取未进行scaledata方法的数据（原始数据）进行训练\n",
    "for train_index, test_index in kf.split(X_1, Label):\n",
    "    print(\"\\n{} of kfold {}\".format(i, kf.n_splits))\n",
    "    X_train, X_test = X_1[train_index], X_1[test_index]\n",
    "    y_train, y_test = Label[train_index], Label[test_index]\n",
    "    ##  使用下面的代码获取不同模型，包括：梯度提升书，极度提升树（论文所使用），随机深林，\n",
    "    # from sklearn import ensemble\n",
    "    # model=ensemble.GradientBoostingClassifier() # 梯度提升树\n",
    "    # model=ensemble.ExtraTreesClassifier()\n",
    "    # model=ensemble.RandomForestClassifier()\n",
    "    ## 使用下面的代码来获取SVM模型(论文所使用)\n",
    "    from sklearn import svm\n",
    "\n",
    "    model = svm.SVC()\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    accuracy = (cm[0, 0] + cm[1, 1]) / sum(sum(cm))\n",
    "    print(accuracy)\n",
    "    #     print(cm)\n",
    "    a.append(accuracy)\n",
    "    i += 1\n",
    "b = np.asarray(a)\n",
    "print(\"五折交叉验证结果为:\", b.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9454f094a5c1e8e6b91f3e6f0db0dad0551253d40628effd67a60000e11016e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
