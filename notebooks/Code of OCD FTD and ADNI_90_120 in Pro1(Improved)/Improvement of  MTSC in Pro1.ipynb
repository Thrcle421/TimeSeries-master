{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuDUnAp0FIoq"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scale data method--我们在论文中提出的按照七种不同缩放方法和四个不同处理方向的预处理策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZfCpAl8UmEJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# #######################\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "\n",
    "\n",
    "def ScaleData(train_x, scaling_method, dimension, random_seed):\n",
    "    if scaling_method == \"none\":\n",
    "        return train_x\n",
    "    if np.isinf(train_x).any():\n",
    "        print(\"Train or test set contains infinity.\")\n",
    "        exit(-1)\n",
    "    scaling_dict = {\n",
    "        \"minmax\": MinMaxScaler(),\n",
    "        \"maxabs\": MaxAbsScaler(),\n",
    "        \"standard\": StandardScaler(),\n",
    "        \"robust\": RobustScaler(),\n",
    "        \"quantile\": QuantileTransformer(random_state=random_seed),\n",
    "        \"powert\": PowerTransformer(),\n",
    "        \"normalize\": Normalizer(),\n",
    "    }\n",
    "    if scaling_method not in scaling_dict.keys():\n",
    "        print(f\"Scaling method {scaling_method} not found.\")\n",
    "        exit(-1)\n",
    "    if dimension not in [\"timesteps\", \"channels\", \"all\", \"both\"]:\n",
    "        print(f\"Dimension {dimension} not found.\")\n",
    "        exit(-1)\n",
    "\n",
    "    dim1 = -1\n",
    "    dim2 = 1\n",
    "    if scaling_method == \"normalize\":\n",
    "        dim1 = 1\n",
    "        dim2 = -1\n",
    "    out_train_x = np.zeros_like(train_x, dtype=np.float64)\n",
    "\n",
    "    train_shape = train_x.shape\n",
    "    if dimension == \"all\":\n",
    "        out_train_x = (\n",
    "            scaling_dict[scaling_method]\n",
    "            .fit_transform(train_x.reshape((dim1, dim2)))\n",
    "            .reshape(train_shape)\n",
    "        )\n",
    "    else:\n",
    "        if dimension == \"channels\":\n",
    "            train_channel_shape = train_x[:, 0, :].shape\n",
    "            for i in range(train_x.shape[1]):\n",
    "                out_train_x[:, i, :] = (\n",
    "                    scaling_dict[scaling_method]\n",
    "                    .fit_transform(train_x[:, i, :].reshape((dim1, dim2)))\n",
    "                    .reshape(train_channel_shape)\n",
    "                )\n",
    "\n",
    "        elif dimension == \"timesteps\":\n",
    "            train_timest_shape = train_x[:, :, 0].shape\n",
    "\n",
    "            for i in range(train_x.shape[2]):\n",
    "                out_train_x[:, :, i] = (\n",
    "                    scaling_dict[scaling_method]\n",
    "                    .fit_transform(train_x[:, :, i].reshape((dim1, dim2)))\n",
    "                    .reshape(train_timest_shape)\n",
    "                )\n",
    "\n",
    "        elif dimension == \"both\":\n",
    "            train_both_shape = train_x[:, 0, 0].shape\n",
    "            for i in range(train_x.shape[1]):\n",
    "                for j in range(train_x.shape[2]):\n",
    "                    out_train_x[:, i, j] = (\n",
    "                        scaling_dict[scaling_method]\n",
    "                        .fit_transform(train_x[:, i, j].reshape((dim1, dim2)))\n",
    "                        .reshape(train_both_shape)\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            print(f\"Dimension {dimension} not found.\")\n",
    "            exit(-1)\n",
    "    return out_train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdoT5PosQBga"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import keras.backend as K\n",
    "import keras.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import LSTM, BatchNormalization, Bidirectional, Dense, Dropout\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from numpy import newaxis\n",
    "\n",
    "\n",
    "##周期性学习率衰减，调试用\n",
    "def scheduler(epoch):\n",
    "    # 每隔5个epoch，学习率减小为原来的1/10-lstm\n",
    "    # if epoch % 5 == 0 and epoch != 0:\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        # lr = K.get_value(model.optimizer.lr*0.001)#LSTM\n",
    "        lr = K.get_value(model.optimizer.lr * 10)  # 一维卷积\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr changed to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "\n",
    "reduce_lr = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9qbbGqVQJvs"
   },
   "outputs": [],
   "source": [
    "def built_model_1(Num_Class=None):\n",
    "    model = Sequential()  # layers [128,64,32,16,4]\n",
    "    model.add(LSTM(input_shape=(None, 90), units=200, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(input_shape=(None, 200), units=128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(input_shape=(None, 128), units=64, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=Num_Class))\n",
    "    # model.add(Activation(\"linear\"))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    start = time.time()\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",  # 加速神经网络\n",
    "        loss=\"categorical_crossentropy\",  # 损失函数\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN 一维卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNDTSZIqQMNU"
   },
   "outputs": [],
   "source": [
    "def built_model_2(Num_Class=None):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Convolution1D(128, 3, strides=1))\n",
    "    model.add(layers.LayerNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(layers.Convolution1D(256, 3, strides=1))\n",
    "    model.add(layers.LayerNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(layers.Convolution1D(128, 3, strides=1))\n",
    "    model.add(layers.LayerNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    model.add(Dense(Num_Class, activation=\"softmax\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiyydHlvQTcQ"
   },
   "outputs": [],
   "source": [
    "def built_model_3(dropout_rate=0.25, activation=\"relu\", Num_Class=None):\n",
    "    start_neurons = 512\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(start_neurons, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 2, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 4, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 8, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate / 2))\n",
    "\n",
    "    model.add(Dense(Num_Class, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",  # 加速神经网络\n",
    "        loss=\"categorical_crossentropy\",  # 损失函数\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USe the follow code to get ADNI_90_120 Data and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1670312983950,
     "user": {
      "displayName": "Levi Ack",
      "userId": "16387227878980052345"
     },
     "user_tz": -480
    },
    "id": "5AuAQFUM1pMq",
    "outputId": "c49435af-54b5-4452-8d2c-d69673e42d1d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as io\n",
    "\n",
    "y = io.loadmat(\"D:/机器学习前沿实验/实验课一/dataset/ADNI_90_120_fMRI.mat\")\n",
    "##AD\n",
    "AD = np.asarray(y[\"AD\"])\n",
    "AD_label = np.zeros([AD.shape[0], 1], dtype=int)\n",
    "##MCI\n",
    "EMCI = np.asarray(y[\"EMCI\"])\n",
    "LMCI = np.asarray(y[\"LMCI\"])\n",
    "MCI = np.vstack((EMCI, LMCI))\n",
    "MCI_label = np.ones([MCI.shape[0], 1], dtype=int)\n",
    "##NC\n",
    "NC = np.asarray(y[\"NC\"])\n",
    "NC_label = np.full((NC.shape[0], 1), 2, dtype=int)\n",
    "\n",
    "##合并:\n",
    "print(AD_label.shape)\n",
    "print(MCI_label.shape)\n",
    "print(NC_label.shape)\n",
    "\n",
    "\n",
    "Data = np.vstack((AD, MCI, NC))\n",
    "Data = Data[:, :, 10:110]\n",
    "Label = np.vstack((AD_label, MCI_label, NC_label))\n",
    "print(Data.shape)\n",
    "print(Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USe the follow code to get OCD Data and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nDmNtcxPsMh"
   },
   "outputs": [],
   "source": [
    "# import scipy.io as io\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# y=io.loadmat(\"D:/机器学习前沿实验/实验课一/dataset/OCD_90_200_fMRI.mat\")\n",
    "# FTD=np.asarray(y['OCD'])\n",
    "# FTD_lable=np.full((FTD.shape[0],1),0,dtype=int)\n",
    "# NC=np.asarray(y['NC'])\n",
    "# NC_lable=np.full((NC.shape[0],1),1,dtype=int)\n",
    "# m=FTD.shape[1]\n",
    "# n=FTD.shape[2]\n",
    "\n",
    "# x = FTD.reshape(FTD.shape[0],-1)\n",
    "# y = NC.reshape(NC.shape[0],-1)\n",
    "\n",
    "# Label_1=np.vstack((FTD_lable,NC_lable))\n",
    "# Data_1=np.vstack((x,y))\n",
    "\n",
    "# Label=Label_1.reshape(Label_1.shape[0])\n",
    "# Data=(Data_1.reshape(-1,m, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USe the follow code to get FTD Data and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xLsuGuwPy3I"
   },
   "outputs": [],
   "source": [
    "# import scipy.io as io\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# y=io.loadmat(\"D:/机器学习前沿实验/实验课一/dataset/FTD_90_200_fMRI.mat\")\n",
    "# FTD=np.asarray(y['FTD'])\n",
    "# FTD_lable=np.full((FTD.shape[0],1),0,dtype=int)\n",
    "# NC=np.asarray(y['NC'])\n",
    "# NC_lable=np.full((NC.shape[0],1),1,dtype=int)\n",
    "# m=FTD.shape[1]\n",
    "# n=FTD.shape[2]\n",
    "\n",
    "# x = FTD.reshape(FTD.shape[0],-1)\n",
    "# y = NC.reshape(NC.shape[0],-1)\n",
    "\n",
    "# Label_1=np.vstack((FTD_lable,NC_lable))\n",
    "# Data_1=np.vstack((x,y))\n",
    "\n",
    "# Data=(Data_1.reshape(-1,m, n))\n",
    "# Label=Label_1.reshape(Label_1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer the Label for later training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAKJNMA8P7FK"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Label = to_categorical(Label)\n",
    "\n",
    "x = Data.transpose((0, 2, 1))\n",
    "x = np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670333713774,
     "user": {
      "displayName": "Levi Ack",
      "userId": "16387227878980052345"
     },
     "user_tz": -480
    },
    "id": "QFk8rA0ARNmA",
    "outputId": "3ac936e7-093a-45ee-d368-023603f58e33"
   },
   "outputs": [],
   "source": [
    "x.shape, Label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五折交叉验证，利用注释处不同代码获得不同效果/不同数据集/是否使用scaledata策略，scaledata策略参数等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U5WB3wryP7Vo"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=100)  # 5折交叉验证\n",
    "\n",
    "i = 1\n",
    "a = []\n",
    "x = np.asarray(x)\n",
    "\n",
    "h = ScaleData(x, \"powert\", \"both\", 100)  ##进行Scaledata操作使用改代码\n",
    "# h=x   ##不进行Scaledata操作使用该代码\n",
    "\n",
    "for train_index, test_index in kf.split(h, Label):\n",
    "    print(\"\\n{} of kfold {}\".format(i, kf.n_splits))\n",
    "    X_train, X_test = h[train_index], h[test_index]\n",
    "    y_train, y_test = Label[train_index], Label[test_index]\n",
    "\n",
    "    ## 使用下面的代码获取模型：built_model_1为LSTM，built_model_2位FCN（一维卷积）built_model_3(为MLP)\n",
    "    model = built_model_1(Num_Class=3)  ##构建模型时输入类别，ADNI为3，OCD,FTD为2\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=16,\n",
    "        epochs=500,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[reduce_lr],\n",
    "    )  # 无scale\n",
    "    x = (np.asarray(history.history[\"accuracy\"]),)\n",
    "    y = np.asarray(history.history[\"val_accuracy\"])\n",
    "    b = np.asarray(y)\n",
    "    print(\"best accuracy os this circle:\", b.max())\n",
    "    a.append(b.max())\n",
    "    i += 1\n",
    "#     model.save(f\"1dConv-Fold{i}\")  ##是否保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打印输出五折交叉验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nheVU97OP7gM",
    "outputId": "7b364b39-4ade-42e0-b937-229f88ade174"
   },
   "outputs": [],
   "source": [
    "a = np.asarray(a)  ##打印输出五折交叉验证结果\n",
    "print(\"五折交叉验证结果:\", a.mean())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNgkCU/mSkGAtb482VIThi5",
   "provenance": [
    {
     "file_id": "1h7zjPev3WTPE-xLbzjuKcMHm24jecQRJ",
     "timestamp": 1669092456929
    },
    {
     "file_id": "1-Jv3XMs-r_QE5yFImhI01eOphFL0gIfm",
     "timestamp": 1667535827752
    },
    {
     "file_id": "1Xg-O502pA9_x80abMF_tP-tYM530rse9",
     "timestamp": 1666692735210
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9454f094a5c1e8e6b91f3e6f0db0dad0551253d40628effd67a60000e11016e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
